# 🤖 머신러닝 학습의 여정: 상세 정리와 미래 계획

## 📊 오늘의 학습 내용 상세 리뷰

### 1. 머신러닝 모델 구축의 기초
#### Q. 학습 과정에서 가졌던 의문점들
- 데이터셋을 효과적으로 로딩하고 모델을 구축하는 전체적인 프로세스는 어떻게 되는가?
- 데이터를 훈련/테스트 세트로 분할하는 최적의 방법과 모델의 훈련 및 평가는 어떻게 진행해야 하는가?

#### A. 오늘 배운 핵심 내용
```python
# 1. 데이터 전처리 과정
# pandas를 활용한 데이터 처리와 train_test_split을 이용한 데이터 분할
import pandas as pd
from sklearn.model_selection import train_test_split

# CSV 파일에서 데이터 로드
df = pd.read_csv('data.csv')
# 특성과 타겟 변수 분리
X = df.drop(columns=['target'])  # 특성 변수
y = df['target']                 # 타겟 변수
# 훈련/테스트 세트 분할 (80:20 비율)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 2. 모델 생성과 학습 프로세스
# RandomForestClassifier를 활용한 모델 구축
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X_train, y_train)  # 모델 학습 실행
```

### 2. 하이퍼파라미터 최적화 전략
다음은 두 가지 주요 최적화 방법의 특징을 자세히 비교한 표입니다:

| 최적화 방법 | 장점 | 단점 | 권장 사용 상황 |
|------------|------|------|----------------|
| GridSearchCV | - 모든 파라미터 조합을 철저히 탐색<br>- 최적값을 확실히 찾을 수 있음 | - 계산 시간이 매우 오래 걸림<br>- 컴퓨팅 리소스 많이 필요 | 파라미터 공간이 작을 때 |
| RandomizedSearchCV | - 효율적인 파라미터 탐색<br>- 빠른 실행 시간 | - 최적값을 놓칠 가능성 존재<br>- 완벽한 최적화 보장 못함 | 큰 파라미터 공간 탐색 시 |

```python
# GridSearchCV를 활용한 하이퍼파라미터 최적화 예제
from sklearn.model_selection import GridSearchCV

# 탐색할 파라미터 정의
param_grid = {
    'n_estimators': [10, 50, 100],  # 트리의 개수
    'max_depth': [None, 10, 20]     # 트리의 최대 깊이
}

# GridSearchCV 객체 생성 및 5-fold 교차 검증 설정
grid_search = GridSearchCV(RandomForestClassifier(), 
                         param_grid, 
                         cv=5,
                         verbose=2)  # 진행 상황 출력
```

### 3. 모델 성능 평가 프로세스
아래 다이어그램은 전체적인 모델 평가 흐름을 보여줍니다:

```mermaid
graph LR
    A[원본 데이터] --분할--> B[훈련 데이터셋]
    A --분할--> C[테스트 데이터셋]
    B --모델 학습--> D[학습된 모델]
    D --예측--> E[성능 평가]
    C --실제값--> E
    E --분석--> F[성능 지표 산출]
```

#### 주요 성능 평가 지표 상세 설명
분류 모델 평가 지표:
- Accuracy: 전체 예측 중 정확한 예측의 비율
- Precision: 양성으로 예측한 것 중 실제 양성의 비율
- Recall: 실제 양성 중 양성으로 예측한 비율
- F1-Score: Precision과 Recall의 조화평균

회귀 모델 평가 지표:
- MSE (Mean Squared Error): 예측값과 실제값 차이의 제곱 평균
- MAE (Mean Absolute Error): 예측값과 실제값 차이의 절대값 평균
- RMSE (Root Mean Squared Error): MSE의 제곱근, 실제 단위로 해석 가능

### 4. 과적합 방지를 위한 효과적인 전략
다음은 모델의 과적합을 방지하기 위한 주요 기법들입니다:

1. 데이터 증강 기법
   - 회전, 이동, 크기 조정 등을 통한 데이터 다양화
   - 노이즈 추가를 통한 강건성 향상

2. 정규화 기법
   - L1 정규화 (Lasso): 불필요한 특성을 제거
   - L2 정규화 (Ridge): 가중치를 작게 유지

3. 교차 검증 전략
   - k-fold 교차 검증
   - 계층적 교차 검증

4. 딥러닝 특화 기법
   - Dropout: 뉴런을 임의로 비활성화
   - Batch Normalization: 레이어 정규화

### 5. 고급 피처 엔지니어링 기법
```python
# 효과적인 피처 선택 예제
from sklearn.feature_selection import SelectKBest
# 상위 10개의 중요 특성 선택
selector = SelectKBest(k=10)
X_new = selector.fit_transform(X, y)

# 데이터 스케일링 적용
from sklearn.preprocessing import StandardScaler
# 평균 0, 표준편차 1로 정규화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### 6. 딥러닝 모델 구축의 기초
```python
# Keras를 활용한 딥러닝 모델 구축
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 순차적 모델 정의
model = Sequential([
    Dense(64, activation='relu', input_shape=(input_dim,)),  # 입력층
    Dense(32, activation='relu'),                           # 은닉층
    Dense(1, activation='sigmoid')                          # 출력층
])

# 모델 컴파일 설정
model.compile(optimizer='adam',
             loss='binary_crossentropy',
             metrics=['accuracy'])
```

## 📚 향후 심화 학습 계획

### 1. 데이터 전처리 고급 과정
- **Pandas/Numpy 고급 기능 마스터**
  - 복잡한 데이터 처리 및 변환
  - 효율적인 메모리 관리 기법
  - 병렬 처리 활용법

- **결측치/이상치 처리 전문가 되기**
  - 다양한 결측치 대체 기법
  - 이상치 탐지 알고리즘 연구
  - 데이터 품질 평가 방법론

### 2. 모델 최적화 심화 과정
- **Optuna 프레임워크 완벽 마스터**
  - 베이지안 최적화 이해
  - 병렬 학습 구현
  - 커스텀 최적화 전략 개발

- **하이퍼파라미터 튜닝 자동화**
  - AutoML 파이프라인 구축
  - 메타러닝 기법 활용
  - 효율적인 탐색 공간 정의

### 3. 실전 배포 및 서비스화
- **웹 서비스 구축**
  - Streamlit 대시보드 개발
  - Flask API 서버 구축
  - 실시간 예측 시스템 구현

- **모델 최적화 및 경량화**
  - 모델 압축 기법
  - 추론 시간 최적화
  - 리소스 사용 효율화

### 4. 고급 성능 개선 전략
- **앙상블 학습 마스터**
  - 다양한 앙상블 기법 연구
  - 스태킹 모델 구현
  - 투표 기반 앙상블 최적화

- **고급 교차 검증 기법**
  - 시계열 교차 검증
  - 중첩 교차 검증
  - 커스텀 검증 전략 개발